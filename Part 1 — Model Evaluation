# Part 1 ‚Äî Model Evaluation
# 1Ô∏è‚É£ Libraries
# ============================================
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, KFold, cross_validate
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import (
    accuracy_score,
    f1_score,
    roc_auc_score
)


# ============================================
# 2Ô∏è‚É£ Load Dataset (Kaggle path)
# ============================================
df = pd.read_csv("/kaggle/input/titanic-dataset/Titanic-Dataset.csv")

df = df[["Survived", "Pclass", "Sex", "Age", "Fare"]]

df["Age"] = df["Age"].fillna(df["Age"].mean())
df["Sex"] = df["Sex"].map({"male": 0, "female": 1})

X = df.drop("Survived", axis=1)
y = df["Survived"]


# ============================================
# 3Ô∏è‚É£ Train‚ÄìTest Split
# ============================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)


# ============================================
# 4Ô∏è‚É£ Model
# ============================================
model = LogisticRegression(max_iter=1000)


# ============================================
# 5Ô∏è‚É£ Train Model
# ============================================
model.fit(X_train, y_train)


# ============================================
# 6Ô∏è‚É£ Train vs Test Performance (Overfitting)
# ============================================
train_pred = model.predict(X_train)
test_pred = model.predict(X_test)

train_prob = model.predict_proba(X_train)[:, 1]
test_prob = model.predict_proba(X_test)[:, 1]

train_accuracy = accuracy_score(y_train, train_pred)
test_accuracy = accuracy_score(y_test, test_pred)

train_f1 = f1_score(y_train, train_pred)
test_f1 = f1_score(y_test, test_pred)

train_auc = roc_auc_score(y_train, train_prob)
test_auc = roc_auc_score(y_test, test_prob)


# ============================================
# 7Ô∏è‚É£ K-Fold Cross Validation (5-Fold)
# ============================================
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

scoring = {
    "accuracy": "accuracy",
    "f1": "f1",
    "roc_auc": "roc_auc"
}

cv_results = cross_validate(
    model,
    X,
    y,
    cv=kfold,
    scoring=scoring,
    return_train_score=True
)


# ============================================
# 8Ô∏è‚É£ CV Performance Summary
# ============================================
cv_summary = {
    "Metric": ["Accuracy", "F1 Score", "ROC-AUC"],
    "CV Mean": [
        cv_results["test_accuracy"].mean(),
        cv_results["test_f1"].mean(),
        cv_results["test_roc_auc"].mean()
    ],
    "CV Std": [
        cv_results["test_accuracy"].std(),
        cv_results["test_f1"].std(),
        cv_results["test_roc_auc"].std()
    ]
}

cv_df = pd.DataFrame(cv_summary)


# ============================================
# 9Ô∏è‚É£ Final Metric Comparison Table
# ============================================
comparison_df = pd.DataFrame({
    "Metric": ["Accuracy", "F1 Score", "ROC-AUC"],
    "Train": [train_accuracy, train_f1, train_auc],
    "Test": [test_accuracy, test_f1, test_auc],
    "CV Mean": [
        cv_results["test_accuracy"].mean(),
        cv_results["test_f1"].mean(),
        cv_results["test_roc_auc"].mean()
    ]
})


# ============================================
# üîü Print Results
# ============================================
print("===== Train vs Test Performance =====")
print(comparison_df)

print("\n===== Cross-Validation Performance =====")
print(cv_df)

print("\n===== Bias‚ÄìVariance Analysis =====")

if train_accuracy - test_accuracy < 0.05:
    print("‚úî Low variance (no overfitting)")
else:
    print("‚ùå High variance (overfitting detected)")

if test_accuracy < 0.75:
    print("‚ö† Possible high bias (model may be too simple)")
else:
    print("‚úî Bias acceptable")


